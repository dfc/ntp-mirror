<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">
<meta name="generator" content="HTML Tidy, see www.w3.org">
<title>How NTP Works</title>
<link href="scripts/style.css" type="text/css" rel="stylesheet">
</head>
<body>
<h3>How NTP Works</h3>
<p>Last update:
  <!-- #BeginDate format:En2m -->16-Sep-2010  19:12<!-- #EndDate -->
  UTC</p>
<h4>Table of Contents</h4>
<ul>
  <li class="inline"><a href="#intro">Introduction</a></li>
  <li class="inline"><a href="#ntp">NTP Daemon Architecture and Basic Operation</a></li>
  <li class="inline"><a href="#stats">How Statistics are Determined</a></li>
  <li class="inline"><a href="#clock">Clock Discipline Algorithm</a></li>
  <li class="inline"><a href="#state">Clock State Machine</a></li>
</ul>
<hr>
<h4 id="intro">Introduction</h4>
<p>NTP time synchronization services are widely available in the public Internet. The public NTP subnet in late 2010 includes several thousand servers in most countries and on every continent of the globe, including Antarctica, and sometimes in space and on the sea floor. These servers support a total population estimated at over 25 million computers in the global Internet.</p>
<p> The NTP subnet operates with a hierarchy of levels, where each level is assigned a number called the stratum. Stratum 1 (primary) servers at the lowest level are directly synchronized to national time services. Stratum 2 (secondary) servers at the next higher level are synchronize to stratum 1 servers and so on. Normally, NTP clients and servers with a relatively small number of clients do not synchronize to public primary servers. There are several hundred public secondary servers operating at higher strata and are the preferred choice.</p>
<p>This page preetns an overview of the NTP daemon included in this distribution. We refer to this as the reference implementation only because it was the one used to test and validate the NTPv4 specificatioin RFC-5905. It is best read in conjunction with the briefings on the <a href="http://www.eecis.udel.edu/~mills/ntp.html">Network Time Synchronization Research Project</a> page.</p>
<div align="center">
  <p><img src="pic/fig_3_1.gif" alt="gif"></p>
  <p>Figure 1. NTP Daemon Processes and Algorithms</p>
</div>
<h4 id="ntp">NTP Daemon Architecture and Basic Operation</h4>
<p>The overall organization of the NTP daemon is shown in Figure 1. It is useful in this context to consider the daemon as both a client of downstatum servers and as a server for upstratum clients. It includes a pair of peer/poll processes for each reference clock or remote  server used as a synchronization source. The poll process sends NTP packets at intervals ranging from 8 s to 36 h. The peer process receives NTP packets and runs the on-wire protocol that collects four timestamps: the <em>origin timestamp</em> <em>T</em><sub>1</sub> upon departure of the client request and the <em>receive timestamp</em> <em>T</em><sub>2</sub> upon arrival at the server, the <em>transmit timestamp</em> <em>T</em><sub>3</sub> upon departure of the server reply and the <em>destination timestamp</em> <em>T</em><sub>4</sub> upon arrival at the client. These timestamps are used to calculate the clock offset and roundtrip delay:</p>
<div align="center">
  <p>offset = [(<em>T</em><sub>2</sub> -<em>T</em><sub>1</sub>) + (<em>T</em><sub>3</sub> - <em>T</em><sub>4</sub>)] / 2<br>
    delay = (<em>T</em><sub>4</sub> - <em>T</em><sub>1</sub>) - (<em>T</em><sub>3</sub> - <em>T</em><sub>2</sub>).</p>
</div>
<p>Those sources that have passed a number of sanity checks are declared <em>selectable</em>. From the selectable population the statistics are used by the select algorithm to determine a number of <em>truechimers</em> according to correctness principles. From the truechimer population a number of <em>survivors</em> are determined on the basis of statistical principles. One of the survivors is declared the <em>system peer</em> and the system statistics  inherited from it. The combine algorithm computes a weighted average of the peer offset and jitter to produce the final values used by the clock discipline algorithm to adjust the system clock time and frequency.</p>
<p> When started, the program requires several measurements sufficient data fro these a algorithms to work properly before setting the clock. As the default poll interval is 64 s, it can take several minutes to set the clock. The time can be reduced using the <tt>iburst</tt> option on the <a href="confopt.html">Server Options</a> page. For additional details about the clock filter, select, cluster and combine algorithms see the Architecture Briefing on the NTP Project Page.</p>
<h4 id="stats">How Statistics are Determined</h4>
<p>Each source is characterized by the  offset and  delay measured by the on-wire protocol and the  dispersion and jitter calculated by the clock filter algorithm of the peer process. Each time an NTP packet is received from a source, the dispersion is initialized by the sum of the precisions of the server and client.</p>
<div align="center"><p><table width="714" border="1" cellpadding="5 p">
  <tr>
    <th width="229" scope="col">Name</th>
    <th width="158" scope="col">Hardware</th>
    <th width="163" scope="col">System</th>
    <th width="104" scope="col">Precision</th>
  </tr>
  <tr>
    <td>deacon.udel.edu</td>
    <td>Sun Blade 1500</td>
    <td>SunOS/5.10</td>
    <td>-20</td>
  </tr>
  <tr>
    <td>howland.udel.edu</td>
    <td>i386 2.8 GHZ dual core</td>
    <td>SunOS/5.11</td>
    <td>-21</td>
  </tr>
  <tr>
    <td>beauregard.udel.edu</td>
    <td>i386 2.8 GHZ dual core</td>
    <td>FreeBSD/8.0</td>
    <td>-19</td>
  </tr>
  <tr>
    <td>pogo.udel.edu</td>
    <td>UltraSPARC 5</td>
    <td>SunOS/5.10</td>
    <td>-20</td>
  </tr>
  <tr>
    <td>huey.usel.edu</td>
    <td>i386</td>
    <td>SunOS/5.11</td>
    <td>-22</td>
  </tr>
  <tr>
    <td>rackety.udel.edu</td>
    <td>i386/Pentium II</td>
    <td>FreeBSD/6.1</td>
    <td>-18</td>
  </tr>
  <tr>
    <td>tick.usno.navy.mil</td>
    <td>HP ia64</td>
    <td>HP-UX/11.31</td>
    <td>-20</td>
  </tr>
  <tr>
    <td>Meinberg LANtime M600 GPS</td>
    <td>i586</td>
    <td>Linux/2.6.15</td>
    <td>-19</td>
  </tr>
  <tr>
    <td>time1.chu.nrc.ca</td>
    <td>i686</td>
    <td>Linux/2.6.27</td>
    <td>-20</td>
  </tr>
</table></p>
<p>Table 1. Typical Precision for Various Machines</p></div>
<p> The offset, delay and dispersion  values are inserted as the youngest stage of  an 8-stage shift register, thus discarding the oldest stage. Subsequently, the dispersion in each stage is increased at a fixed rate of 15 <font face="symbol">m</font>s/s, representing the worst case error due to skew between the server and client clocks.  The clock filter algorithm in each peer process selects the stage with the lowest delay, which generally represents the most accurate values, and the associated offset and delay   values become the peer variables of the same name. The peer dispersion continues to grow at the same  rate as the register dispersion. The peer dispersion  is determined as a weighted average of the dispersion samples in the shift register. Finally, the peer jitter is determined as the root-mean-square (RMS) average of all the offset samples in the shift register relative to the selected sample. </p>
<p> The clock filter algorithm continues to process packets in this way until the source is no longer reachable. In this case the algorithm inserts dummy samples with  &quot;infinite&quot; dispersion are inserted in the shift register, thus displacing old samples.</p>
<p>The composition of  the survivor population and the system peer selection is redetermined as each update from each server is received. The system variables are copied from the  peer variables of the same name and the system stratum set one greater than the system peer stratum. Like  peer dispersion, the system dispersion increases at the same rate so, even if all sources have become unreachable, the daemon appears to upstratum clients at ever increasing dispersion.</p>
<h4>Reachability and Selection Criteria</h4>
<p>Of interest in this discussion is how an NTP client determines if a server is reachable and suitable as a source of synchronization. Reachability is determined by an 8-bit shift register, which is shifted left by one bit as each poll message is sent, with zero replacing the vacated rightmost bit. Each time an update is received the rightmost bit is set. The source is considered reachable if any bit is set in the register; otherwise, it is considered unreachable. The peer dispersion is used as a filter to determine whether a source is usable or not.  If the server is unreachable or the dispersion exceeds the select threshold threshold (1.5 s by default),  it is not  selectable to synchronize the system clock.</p>
<p>The quality of time service is determined by a quantity called the <em>synchronization distance.</em> It is  computed as one-half the <em>root delay</em> to the primary source of time; i.e., the primary reference clock, plus the <em>root dispersion</em>.   The root root delay and root dispersion are included in the NTP packet header. The client adds the current system peer delay and dispersion to the corresponding root values and saves them in its own system variables which are passed on to dependent clients.   Like peer dispersion, the  root dispersion increases at the same rate.</p>
<p>Although it might seem counterintuitive, a cardinal rule in the selection processes is, once a sample has been selected by the clock filter algorithm, that sample and any older samples are no longer selectable. This applies also to the select algorithm. Once the peer variables for a source have been selected, older variables of the same or other sources are no longer selectable. This means that not every sample can be used to update the peer variables and up to seven samples can be ignored between selected samples. This fact has been carefully considered in the discipline algorithm design with due consideration of the feedback loop delay and minimum sampling rate. In engineering terms, even if only one sample in eight survives, the resulting sample rate is twice the Nyquist rate at any time constant and poll interval.</p>
<h4 id="clock">Clock Discipline Algorithm</h4>
<p>At the heart of the NTP host is the clock discipline algorithm, which is best described as a hybrid phase/frequency-lock feedback loop. In the NTP reference implementation, it is an intricately crafted algorithm that automatically  adapts  for optimum performance while  minimizing network overhead. Its response  is determined by  the <em>time constant</em>, which results in a &quot;stiffness&quot; depending on the   jitter  of the available sources and the stability of the system clock oscillator. The  time constant is also used as the poll interval  by the poll processes. However, in  NTP symmetric mode, each   peer manages its own poll interval and the two  might not be the same. In such cases either peer uses the minimum of its own poll interval and that of the other peer, which is included in the NTP  packet header.</p>
<p>It is important to understand how the dynamics of the discipline are affected by the poll interval. At an interval of 64 s and a step change of 100 ms, the time response crosses zero in about 50 minutes and overshoots about 7 ms, as per design. However, the step correction causes a temporary frequency  change of about 5 PPM, which is slowly corrected within a few hours. The result is that the overshoot slowly subsides over the that interval. This is an intrinsic feature of a discipline loop that can minimize both time and frequency offset.</p>
<p>Since the discipline loop is linear, the response with different step sizes and poll intervals has the characteristic, but scaled differently in amplitude and time. The response scales exactly with step amplitude, so that the response to  a 10-ms step has the same characteristic, but with amplitude one-tenth that with the 100-ms step. The response scales exactly with poll interval, so at a poll interval of 8 s the time response is the same as at 64 s, but at a time scale one-eighth that at 64 s.</p>
<p>In the NTP specification and reference implementation, poll intervals are  expressed as exponents of 2; thus, a poll exponent of 6 represents an actual poll interval; of 64 s. The clock discipline time constant is also expressed in powers of 2 and, with proper scaling, the two have the same value. Thus, a change in one corresponds to the same change in the other.</p>
<p> The optimum time constant, and thus the poll interval, depends on the network time jitter and the clock oscillator frequency wander. Errors due to jitter decrease as the time constant increases, while errors due to wander decrease as the time constant decreases. The two error characteristics intersect at a point called the Allan intercept, which represents the ideal time constant. With a compromise Allan intercept of 2000 s, the optimum poll interval is about 64 s, which corresponds to a poll exponent of 6.</p>
<p> The poll interval is managed by a heuristic algorithm developed over several years of experimentation. It depends on an exponentially weighted  average of clock offset differences, called the clock jitter, and a jiggle counter, which is initially set to zero.  When a clock update is received and the offset exceeds the clock jitter by a factor of 4,   the jiggle counter is increased by the poll exponent; otherwise, it is decreased by twice the poll exponent. If  the jiggle counter is greater than an arbitrary threshold of 30,  the poll exponent. if jiggle counter exceed an arbitrary threshold of 30, it is reset to zero and the poll exponent increased by 1.  If the jiggle counter is less than -30, it is set to zero and the  poll exponent  decreased by 1. In effect, the algorithm has a relatively slow reaction to good news, but a relatively fast reaction to bad news.</p>
<p>The optimum time constant, and thus the poll exponent, depends on the network time jitter and the clock oscillator frequency wander. Errors due to jitter decrease as the time constant increases, while errors due to wander decrease as the time constant decreases. The two error characteristics intersect at a point called the Allan intercept, which represents the ideal time constant. With a compromise Allan intercept of 2000 s, the optimum poll interval is about 64 s, which corresponds to a poll exponent of 6.</p>
<h4 id="state">Clock State Machine</h4>
<p>In the NTPv4 specification and reference implementation a state machine is used to manage the system clock under exceptional conditions, as when the daemon is first started or when encountering  severe network congesiton, for example. The state machine uses three thresolds: panic, step and stepout, and a watchdog timer. The thresholds default to 1000 s, 128 ms and 900 s, respectively, but can be changed by command options.</p>
<p>Most compters today incorporate a time-of-year (TOY) chip to maintain the time  when the power is off. When the machine is restarted, the chip is used to initialize the operating system time. In case there is no TOY chip or the TOY&nbsp;time is different from  NTP  time by more than the  panic threshold, the daemon <tt></tt> assumes something must be terribly wrong, so exits with a message to the system operator to set the time  manually. With the <tt>-g</tt> option, the daemon will set the clock  to NTP time the first time, but exit if the offset exceed the any time after that.</p>
<p>Under ordinary conditions, the clock discipline slews the clock so that the time is effectively continuous and never runs backwards. If due to extreme network congestion, or an offset spike exceeds the  step threshold, by default 128 ms, the spike is discarded. However, if   offsets greater than the step threshold persist for more than the <i>stepout threshold</i>, by default 900 s, the system clock is stepped to the correct value. In practice the need for a step has been extremely rare and almost always the result of a hardware failure.  Both the step threshold and stepout threshold can be set as options to the tinker command.</p>
<p>Historically, the most important appliccation of the step function was when a leap second was inserted in the Coordinated Univesal Time (UTC) timescale and kernel precision time support was not available. Further details are on the <a href="leap.html">Leap Second Processing</a> page.</p>
<p>In some applications the clock can never be set backward, even  it  accidentlly set forward a week by some other means. There are several ways to alter the daemon behavior to insure time is always monotone-increasing. If the step threhold is set to zero, there will never be a step.  With the <tt>-x</tt> command line option the daemon will set will set the step threshold  to 600 s, which is about the limit of eyeball and wristwatch. However, in any of these cases, the precision time kernel support is disabled, as it cannot handle offsets greater than &plusmn;0.5 s.</p>
<p>The issues should be carefully considered before using these options. The  slew rate   is fixed at  500 parts-per-million (PPM) by the Unix kernel. As a result, the clock can take 33 minutes to amortize  each second the clock is outside the acceptable range. During this interval the clock will not be consistent with any other network clock and the system cannot be used for distributed applications that require correctly synchronized network time.</p>
<p>The frequency file, usually called <tt>ntp.drift</tt>, contains the latest estimate  of clock frequency. If this file does not exist when the daemon is started, the clock state machine enters a special mode designed to measure the particular frequency directly. The measurement takes an interval equal to the stepout threshold, after which the frequency is set and the daemon esumes normal mode where the time and frequency are continuously adjusted. The frequency file is updated at intervals of an hour or more depending on the measured clock stability.</p>
<hr>
<p>
  <script type="text/javascript" language="javascript" src="scripts/footer.txt"></script>
</p>
</body>
</html>
